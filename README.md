OPTIMIZERS IN NEURAL NETWORK


ABOUT THE STRUCTURE OF THE DATASET:->
ðŸ”¹ Dense layers
ðŸ”¹ Activation functions (ReLU & Softmax)
ðŸ”¹ Loss function (Categorical Cross-Entropy)
ðŸ”¹ Optimization (SGD)

Core Components:->
Layer_Dense: Fully connected neural network layer ðŸ”—
Activation_ReLU: ReLU activation (with backpropagation) âš¡
Activation_Softmax: Stable softmax function ðŸ“ˆ
Loss_CategoricalCrossentropy: Categorical loss with gradients ðŸŽ¯
Activation_Softmax_Loss_CategoricalCrossentropy: Efficient combo ðŸ”¥


Training
âœ… Complete forward & backward propagation
âœ… SGD optimizer implementation
ðŸ“Š Accuracy & loss tracking


ðŸ–¼ï¸ Visualization
Spiral data visualization ðŸŒªï¸
Optional training progress plots ðŸ“‰



Key Implementation Details
ðŸ” Forward Propagation
Layer math:
output = inputs @ weights + biases

ReLU:
max(0, x)

Softmax (w/ numerical stability):
exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))
probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)



ðŸ”„ Backward Propagation
GRADIENTS:->
Weights: inputs.T @ dvalues
Biases: np.sum(dvalues, axis=0)
Inputs: dvalues @ weights.T
ReLU derivative: 0 where input < 0
Combined Softmax + CrossEntropy: Optimized for performance âš¡


âš™ï¸ Optimization

(1) GRADIENT DESCENT
Stochastic Gradient Descent (SGD):
layer.weights += -learning_rate * layer.dweights
layer.biases += -learning_rate * layer.dbiases
